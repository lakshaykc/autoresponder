{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda/envs/autoresponder/lib/python3.10/site-packages/datasets/dataset_dict.py:1241: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'storage_options=fs.storage_options' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['prompt', 'ground_truth', 'input_ids', 'attention_mask', 'labels'], dtype='object')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk, Dataset\n",
    "import evaluate\n",
    "\n",
    "full_ds = load_from_disk(\"data/chat_history\")\n",
    "\n",
    "# We only want to evaluate on the test set to ensure we didn't overfit\n",
    "sample_data = full_ds[\"test\"].to_pandas().rename(columns={\"text\": \"ground_truth\"})\n",
    "\n",
    "sample_data = sample_data.sample(n=100, random_state=1)\n",
    "\n",
    "print(len(sample_data))\n",
    "sample_data.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_compare = [\n",
    "    ('t5-lg-base', 'google/flan-t5-large'),\n",
    "    ('t5-lg-finetuned', 'models/flan-t5-large-10ep/checkpoint-42000'),\n",
    "    ('t5-xl-base', 'google/flan-t5-xl'),\n",
    "    ('t5-xl-finetuned', 'models/flan-t5-xl/checkpoint-38000'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7499695cf0b4ec78271fa95bb79fb8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading model t5-xl-base from google/flan-t5-xl...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loading model t5-xl-base from google/flan-t5-xl...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Running inference for t5-xl-base...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Running inference for t5-xl-base...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading model t5-xl-finetuned from models/flan-t5-xl/checkpoint-38000...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loading model t5-xl-finetuned from models/flan-t5-xl/checkpoint-38000...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Running inference for t5-xl-finetuned...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Running inference for t5-xl-finetuned...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading model t5-lg-base from google/flan-t5-large...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loading model t5-lg-base from google/flan-t5-large...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Running inference for t5-lg-base...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Running inference for t5-lg-base...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Running inference for t5-lg-finetuned...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Running inference for t5-lg-finetuned...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, pipeline\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from rich.progress import Progress, MofNCompleteColumn\n",
    "import torch\n",
    "\n",
    "prompts = list(sample_data[\"prompt\"])\n",
    "\n",
    "\n",
    "\n",
    "with Progress(*Progress.get_default_columns(), MofNCompleteColumn()) as progress:    \n",
    "    for model_name, model_path in models_to_compare:\n",
    "        print(f\"Loading model {model_name} from {model_path}...\")\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "        pipe = pipeline(\n",
    "            \"text2text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=\"google/flan-t5-base\",\n",
    "            device='cuda:0',\n",
    "        )\n",
    "        \n",
    "        # pipelines appear to only return iterators when used with a KeyDataset,\n",
    "        # and we have to create a Dataset to create a KeyDataset\n",
    "        prompts_ds = Dataset.from_dict({\"prompt\": prompts})\n",
    "        prompts_ds = KeyDataset(prompts_ds, \"prompt\")\n",
    "\n",
    "        print(f\"Running inference for {model_name}...\")\n",
    "        \n",
    "        predictions_generator = pipe(\n",
    "            prompts_ds,\n",
    "            max_length=50,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=2.0,\n",
    "            num_return_sequences=5,\n",
    "            batch_size=4,\n",
    "        )\n",
    "        all_predictions = []\n",
    "        \n",
    "        for predictions in progress.track(predictions_generator, description=model_name, total=len(prompts)):\n",
    "            all_predictions.append([p[\"generated_text\"] for p in predictions])\n",
    "        \n",
    "        sample_data[model_name] = all_predictions\n",
    "        \n",
    "        # Release GPU memory\n",
    "        model.cpu()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "826c1c29c4fa44dc95ceb33457863a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from diskcache import Cache\n",
    "import time\n",
    "from rich.progress import track\n",
    "\n",
    "# Load the OpenAI API key from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "import openai\n",
    "\n",
    "cache = Cache(\"data/openai_cache\")\n",
    "\n",
    "@cache.memoize(\"oai_infer_dv3_n5_mt200\")\n",
    "def openai_inference(prompt):\n",
    "    completions = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\", prompt=prompt, n=5, max_tokens=200\n",
    "    )\n",
    "\n",
    "    # Watch out for that rate limit\n",
    "    time.sleep(1)\n",
    "    return [completion[\"text\"] for completion in completions[\"choices\"]]\n",
    "\n",
    "\n",
    "openai_inference(\"a man a plan a canal\")\n",
    "\n",
    "sample_data[\"davinci-003\"] = [openai_inference(prompt) for prompt in track(prompts)]\n",
    "\n",
    "# models_to_compare[\"openai\"] = openai_batch_inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        table {\n",
       "            /* Make all columns the same width */\n",
       "            table-layout: fixed;\n",
       "            background-color: #f5f5f5;\n",
       "            \n",
       "            /* break long urls */\n",
       "            word-wrap: break-word;\n",
       "            word-break: break-word;\n",
       "        }\n",
       "        th, td {\n",
       "            max-width: 300px;\n",
       "            text-align: left;\n",
       "            vertical-align: top;\n",
       "            font-family: sans-serif;\n",
       "        }\n",
       "    </style>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>prompt</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>t5-xl-base</th>\n",
       "      <th>t5-xl-finetuned</th>\n",
       "      <th>t5-lg-base</th>\n",
       "      <th>t5-lg-finetuned</th>\n",
       "      <th>davinci-003</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Respond to chat as Kyle Corbitt:<br><br>Recent messages:<br><br>Jacqueline Corbitt:<br>Vanessa's cousin Mikey wants everyone to know that his T-Rex is bigger than Vanessa üòÅ<br><br>David Corbitt:<br>Lucky for him she can't walk yet<br><br>David Corbitt:<br>Merry Christmas<br><br>Scott Corbitt:<br>Shoulda been Bon Nadal my bad<br><br>Scott Corbitt:<br>Hey everybody, you keep promising to post pictures. Let's make that happen (so I can repost some of them to Instagram)<br><br>Christina Corbitt:<br>Merry Christmas üéÑ<br><br>Jacqueline Corbitt:<br>Baby's first stocking<br><br>Jacqueline Corbitt:<br>Vanessa loves her Uncle Kevin, much to the jealousy of Uncle Kevin's own child üòÇ<br><br>Christina Corbitt:<br>We have a white Christmas in Washington üòå<br><br>Karen Corbitt:<br>Oh my.  Are the roads ok?<br><br>Kyle Corbitt:<br></td>\n",
       "      <td>Wow nice!!</td>\n",
       "      <td>The roads are mostly fine.<br>--------------------<br>Karen Corbitt: yes, the roads were cleared yesterday afternoon<br>--------------------<br>Jacqueline Corbitt: Most of them are. We don't have too much traffic compared to last year, but it's not good.<br>--------------------<br>Yes, they are mostly good.<br>--------------------<br>No, we had problems with some streets.</td>\n",
       "      <td>Washington state is a fun place!<br>--------------------<br>Pretty good<br>--------------------<br>Pretty good<br>--------------------<br>Seattle is a pretty good example of how not to treat road salt<br>--------------------<br>Pretty good</td>\n",
       "      <td>I just found out.<br>--------------------<br>Christina Corbitt: You guys are awesome.<br>--------------------<br>Christina Corbitt: No. It's just frozen road ice.<br>--------------------<br>Christmas<br>--------------------<br>Karen Corbitt: I can't believe it's snowing, and I want the roads to be safe.</td>\n",
       "      <td>Well, that's another reason to love Washington <br>--------------------<br>Yep, aren't too worried about the roads here at all. Not sure if they should be closed because of the weather though<br>--------------------<br>Yeah pretty good for Washington!<br>--------------------<br>yes, they're ok!<br>--------------------<br>I mean, everybody said they‚Äôd be fine for the whole trip, right?</td>\n",
       "      <td>Merry Christmas everyone! And Karen, I heard the roads should be okay, but drive safely!<br>--------------------<br>Hey everyone! Wow, what a great conversation! I'm sure Mikey's T-Rex is huge! And yeah, let's post some pics so the rest of the family can see what we're up to. It sounds like Vanessa already has quite the fan club! As for the snow out in Washington, Karen, I'm sure it looks beautiful, but please be careful if you're out on the roads! ü§ó<br>--------------------<br>Ha! That's funny, Jacqueline. Merry Christmas, everyone! Let's see those pictures! Karen, yes, the roads have been cleared.<br>--------------------<br>Merry Christmas to all! It sounds like everyone is having a great holiday. And yes Karen, the roads in Washington should be OK - mostly just snowing here in New York.<br>--------------------<br>Hey everyone! Haha, I love the stories about Vanessa and Uncle Kevin. Definitely wanna see those photos! Karen, roads here in Phoenix are fine. Wish you all a very Merry Christmas!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Respond to chat as Kyle Corbitt:<br><br>Recent messages:<br><br>David Corbitt:<br>I am really excited to try this. They say 46 minutes of flight time but if it works for 30 I'll be happy<br><br>Kyle Corbitt:<br>nice<br><br>Kyle Corbitt:<br></td>\n",
       "      <td>been thinking about buying a drone forever</td>\n",
       "      <td>will you come up?<br>--------------------<br>David Corbitt: They have a lot of great plans but the flights are so expensive.<br>--------------------<br>David Corbitt: The only thing missing is me!<br>--------------------<br>David Corbitt: I don't know about the price, but I am sure it will be a good trip.<br>--------------------<br>David Corbitt: yup, this looks funky.</td>\n",
       "      <td>can you send me a screenshot when it starts recording?<br>--------------------<br>if you think it works will let us know!<br>--------------------<br>let's try it!<br>--------------------<br>how was the experience?<br>--------------------<br>ok 30 isn't that bad, I guess?</td>\n",
       "      <td>glad to know you are having fun<br>--------------------<br>no problem.<br>--------------------<br>is it the one from taht you love?<br>--------------------<br>I'll take it<br>--------------------<br>ok</td>\n",
       "      <td>if they don't give you the option for an actual phone, would that work better?<br>--------------------<br>what's your plan to get there?<br>--------------------<br>what do they call it?<br>--------------------<br>where do you want to fly?<br>--------------------<br>how long will it take you to get from chicago-san bruno airport?</td>\n",
       "      <td>Sounds great, David! I'm excited to hear how it works out for you!<br>--------------------<br>I'm excited to try this too! It sounds like a fun experience. Let's hope it works for at least the full 46 minutes!<br>--------------------<br>That sounds great, David. You should definitely give it a try and let us know how it goes!<br>--------------------<br>That sounds great, David! I'm looking forward to testing it out -- let's aim for at least 30 minutes of flight time and see how it goes.<br>--------------------<br>That sounds great! I'm sure you'll be able to get at least 30 minutes out of it. Good luck with your flight!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pprint\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Pull the first item from the models_to_compare tuples\n",
    "model_cols = [model[0] for model in models_to_compare]\n",
    "model_cols.append(\"davinci-003\")\n",
    "sample_formatted = sample_data.copy()[[\"prompt\", \"ground_truth\", *model_cols]]\n",
    "\n",
    "\n",
    "for col in model_cols:\n",
    "    sample_formatted[col] = sample_formatted[col].apply(\n",
    "        lambda x: f\"\\n{'-'*20}\\n\".join(x)\n",
    "    )\n",
    "\n",
    "def df_to_html(df):\n",
    "    table_html = df.to_html(index=False, escape=False).replace(\"\\\\n\", \"<br>\")\n",
    "    \n",
    "    table_styling = \"\"\"\n",
    "    <style>\n",
    "        table {\n",
    "            /* Make all columns the same width */\n",
    "            table-layout: fixed;\n",
    "            background-color: #f5f5f5;\n",
    "            \n",
    "            /* break long urls */\n",
    "            word-wrap: break-word;\n",
    "            word-break: break-word;\n",
    "        }\n",
    "        th, td {\n",
    "            max-width: 300px;\n",
    "            text-align: left;\n",
    "            vertical-align: top;\n",
    "            font-family: sans-serif;\n",
    "        }\n",
    "    </style>\n",
    "    \"\"\"\n",
    "\n",
    "    return table_styling + table_html\n",
    "\n",
    "display(HTML(df_to_html(sample_formatted.head(2))))\n",
    "\n",
    "with open(\"data/table.html\", \"w\") as f:\n",
    "    f.write(df_to_html(sample_formatted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoresponder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767c9e6bcb6bf1e56b6e269cd77223d5a15d6d7a64e3ff78b92463e57e0451c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
